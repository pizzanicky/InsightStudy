import sys
import os
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from sqlalchemy import select, and_
from loguru import logger

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Setup paths
project_root = Path(__file__).resolve().parents[1]
media_crawler_root = project_root / "MindSpider" / "DeepSentimentCrawling" / "MediaCrawler"

# Add MediaCrawler to sys.path for its internal imports
if str(media_crawler_root) not in sys.path:
    sys.path.append(str(media_crawler_root))

# Import root config.py using importlib to avoid naming conflict
import importlib.util
config_path = project_root / "config.py"
spec = importlib.util.spec_from_file_location("root_config", config_path)
root_config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(root_config)
settings = root_config.settings

# Import database modules
from MindSpider.DeepSentimentCrawling.MediaCrawler.database.db_session import get_session, clear_engine_cache
from MindSpider.DeepSentimentCrawling.MediaCrawler.database.models import WeiboNote

# Import prompt
from DailyDigest.prompts import DAILY_DIGEST_PROMPT

# Import Google Gemini SDK
import google.generativeai as genai

class SimpleLLM:
    """Simple wrapper around Google Gemini API"""
    def __init__(self):
        # Load Google Gemini config from environment
        api_key = os.getenv("GOOGLE_API_KEY")
        model_name = os.getenv("GOOGLE_MODEL_NAME", "gemini-2.0-flash-exp")
        
        if not api_key:
            raise ValueError("GOOGLE_API_KEY is not configured in .env file")
        
        # Configure Google Gemini
        genai.configure(api_key=api_key)
        
        # Initialize the model
        self.model = genai.GenerativeModel(model_name)
        self.model_name = model_name
        
        logger.info(f"[SimpleLLM] Initialized Google Gemini: {model_name}")
    
    def chat(self, prompt: str) -> str:
        """Simple chat interface using Google Gemini"""
        try:
            logger.info(f"[SimpleLLM] Sending request to {self.model_name}")
            
            # Generate content using Gemini
            response = self.model.generate_content(prompt)
            
            # Extract text from response
            if response and response.text:
                logger.info(f"[SimpleLLM] Received response ({len(response.text)} chars)")
                return response.text
            else:
                logger.error("[SimpleLLM] Empty response from Gemini")
                raise ValueError("Empty response from Gemini API")
                
        except Exception as e:
            logger.error(f"[SimpleLLM] Error calling Gemini API: {e}")
            raise

class DailyDigest:
    def __init__(self):
        self.llm = SimpleLLM()
    
    async def crawl_reddit(self, keyword: str, max_count: int = 100):
        """
        çˆ¬å–Redditæ•°æ®
        ä½¿ç”¨subprocessè°ƒç”¨MediaCrawlerï¼Œé¿å…å¯¼å…¥å†²çª
        è¿”å›ž: (success: bool, message: str, post_count: int)
        """
        try:
            logger.info(f"[DailyDigest] Starting Reddit crawl for keyword: {keyword}")
            
            # ä½¿ç”¨subprocessè°ƒç”¨MediaCrawlerçš„çˆ¬è™«
            import subprocess
            import tempfile
            
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            config_content = f"""
PLATFORM = "reddit"
KEYWORDS = "{keyword}"
LOGIN_TYPE = "qrcode"
CRAWLER_TYPE = "search"
CRAWLER_MAX_NOTES_COUNT = {max_count}
SAVE_DATA_OPTION = "postgresql"
"""
            
            # å†™å…¥ä¸´æ—¶é…ç½®
            temp_config_path = media_crawler_root / "config" / "base_config_temp.py"
            original_config_path = media_crawler_root / "config" / "base_config.py"
            
            # å¤‡ä»½åŽŸé…ç½®
            import shutil
            backup_path = media_crawler_root / "config" / "base_config_backup.py"
            shutil.copy(original_config_path, backup_path)
            
            # è¯»å–åŽŸé…ç½®å¹¶æ›´æ–°å…³é”®å‚æ•°
            with open(original_config_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            # ä¿®æ”¹é…ç½®
            import re
            modified_content = original_content
            modified_content = re.sub(r'KEYWORDS = .*', f'KEYWORDS = "{keyword}"', modified_content)
            modified_content = re.sub(r'CRAWLER_MAX_NOTES_COUNT = .*', f'CRAWLER_MAX_NOTES_COUNT = {max_count}', modified_content)
            
            # å†™å…¥ä¿®æ”¹åŽçš„é…ç½®
            with open(original_config_path, 'w', encoding='utf-8') as f:
                f.write(modified_content)
            
            # è¿è¡Œçˆ¬è™«
            cmd = [
                'python3',
                str(media_crawler_root / 'main.py'),
                '--platform', 'reddit',
                '--lt', 'qrcode',
                '--type', 'search',
                '--save_data_option', 'postgresql'
            ]
            
            logger.info(f"[DailyDigest] Running crawler command: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                cwd=str(media_crawler_root),
                capture_output=True,
                text=True,
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            # è®°å½•çˆ¬è™«è¾“å‡ºä»¥ä¾¿è°ƒè¯•
            logger.info(f"[DailyDigest] Crawler STDOUT:\n{result.stdout}")
            if result.stderr:
                logger.warning(f"[DailyDigest] Crawler STDERR:\n{result.stderr}")
            
            # æ¢å¤åŽŸé…ç½®
            shutil.move(backup_path, original_config_path)

            # ðŸš¨ æ£€æŸ¥æ˜¯å¦è¢« Reddit æ‹¦æˆª (403 Forbidden)
            combined_output = (result.stdout or "") + (result.stderr or "")
            crawler_blocked = False
            if "403" in combined_output and ("Block" in combined_output or "whoa there" in combined_output or "Reddit" in combined_output):
                logger.error("[DailyDigest] Crawler detected 403 Forbidden/Blocked response.")
                crawler_blocked = True
            
            # ç®€åŒ–é€»è¾‘ï¼šå¦‚æžœè¢«æ‹¦æˆªï¼Œæˆ–è€…çˆ¬è™«å¤±è´¥ï¼Œæˆ–è€…çˆ¬åˆ°äº†0æ¡æ•°æ®ï¼Œéƒ½å°è¯•ä½¿ç”¨ Tavily
            # Check how many posts were crawled (from DB)
            posts = await self.get_recent_posts(keyword, hours=24)
            post_count = len(posts)
            
            # åªæœ‰å½“æ²¡æœ‰çˆ¬åˆ°æ•°æ®ï¼Œä¸”ï¼ˆè¢«æ‹¦æˆª æˆ– æŠ¥é”™ï¼‰æ—¶ï¼Œæ‰è§¦å‘ Fallback
            # å¦‚æžœå·²ç»çˆ¬åˆ°äº†æ•°æ®(post_count > 0)ï¼Œå³ä½¿æœ‰403è­¦å‘Š(å¯èƒ½æ˜¯éƒ¨åˆ†èµ„æºå¤±è´¥)ï¼Œä¹Ÿè§†ä¸ºæˆåŠŸï¼Œç›´æŽ¥ä½¿ç”¨
            if post_count == 0 and (crawler_blocked or result.returncode != 0):
                logger.warning(f"[DailyDigest] Primary crawler failed/blocked and NO posts found. Attempting fallback to Tavily Search...")
                
                # Fallback to Tavily
                tavily_success, tavily_msg, tavily_count = await self.crawl_reddit_via_tavily(keyword, max_count)
                
                if tavily_success and tavily_count > 0:
                     return True, f"é€šè¿‡ Tavily æœç´¢æˆåŠŸèŽ·å– {tavily_count} æ¡æ•°æ® (åŽŸçˆ¬è™«å¤±æ•ˆ)", tavily_count
                
                # å¦‚æžœ Tavily ä¹Ÿå¤±è´¥
                if crawler_blocked:
                    return False, "âŒ çˆ¬è™«è¢« Reddit æ‹¦æˆªä¸” Tavily æœç´¢æœªæ‰¾åˆ°è¡¥å……æ•°æ®ã€‚\nè¯·æ£€æŸ¥ TAVILY_API_KEY é…ç½®æˆ–æ›´æ¢èŠ‚ç‚¹ã€‚", 0
                
                return False, f"çˆ¬è™«å’Œæœç´¢å‡æœªæ‰¾åˆ°å…³äºŽ '{keyword}' çš„æ–°æ•°æ®", 0
            
            if post_count == 0:
                 return False, f"çˆ¬è™«æ‰§è¡Œå®Œæˆï¼Œä½†æœªæ‰¾åˆ°å…³äºŽ '{keyword}' çš„æ–°å¸–å­", 0


            logger.info(f"[DailyDigest] Crawl completed. Found {post_count} posts for '{keyword}'")
            return True, f"æˆåŠŸçˆ¬å– {post_count} æ¡å¸–å­", post_count
            
        except subprocess.TimeoutExpired:
            logger.error(f"[DailyDigest] Crawler timeout after 120s")
            return False, "çˆ¬å–è¶…æ—¶ï¼ˆè¶…è¿‡2åˆ†é’Ÿï¼‰", 0
        except Exception as e:
            logger.error(f"[DailyDigest] Crawl failed: {e}")
            return False, f"çˆ¬å–å¼‚å¸¸: {str(e)}", 0

    async def crawl_reddit_via_tavily(self, keyword: str, max_results: int = 20):
        """
        Fallback: Use Tavily API to search Reddit when crawler is blocked.
        """
        try:
            from tavily import TavilyClient
            api_key = os.getenv("TAVILY_API_KEY") or settings.TAVILY_API_KEY
            
            if not api_key:
                logger.warning("TAVILY_API_KEY not found, skipping fallback.")
                return False, "æœªé…ç½® Tavily API Key", 0
                
            client = TavilyClient(api_key=api_key)
            
            logger.info(f"[DailyDigest] Searching Tavily for: site:reddit.com {keyword}")
            response = client.search(
                query=f'site:reddit.com "{keyword}"',
                search_depth="advanced",
                max_results=max_results,
                include_raw_content=False
            )
            
            results = response.get('results', [])
            if not results:
                logger.warning("Tavily returned no results.")
                return False, "Tavily æœªæ‰¾åˆ°ç»“æžœ", 0
            
            logger.info(f"[DailyDigest] Tavily found {len(results)} results. Saving to DB...")
            
            saved_count = 0
            async with get_session() as session:
                for item in results:
                    # åˆ›å»ºä¼ªé€ çš„ WeiboNote (Reddit Post)
                    url = item.get('url', '')
                    content = item.get('content', '')
                    title = item.get('title', '')
                    
                    # ç®€å•çš„åŽ»é‡/IDç”Ÿæˆé€»è¾‘
                    import hashlib
                    note_id_hash = int(hashlib.md5(url.encode()).hexdigest(), 16) % (10**16) # ç”Ÿæˆä¸€ä¸ªå¤§æ•´æ•°ID
                    
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    stmt = select(WeiboNote).where(WeiboNote.note_id == note_id_hash)
                    existing = (await session.execute(stmt)).scalar_one_or_none()
                    
                    current_ts = int(datetime.now().timestamp() * 1000)
                    
                    if existing:
                        existing.last_modify_ts = current_ts
                        existing.source_keyword = keyword # æ›´æ–°å…³é”®è¯å…³è”
                    else:
                        new_note = WeiboNote(
                            note_id=note_id_hash,
                            note_url=url,
                            content=f"{title}\n\n{content}", # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦
                            source_keyword=keyword,
                            nickname="RedditUser (Via Tavily)",
                            user_id="tavily_search",
                            avatar="",
                            liked_count="0",
                            comments_count="0",
                            shared_count="0",
                            add_ts=current_ts,
                            last_modify_ts=current_ts,
                            create_time=current_ts,
                            create_date_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        )
                        session.add(new_note)
                        saved_count += 1
                
                await session.commit()
            
            return True, "Success", saved_count

        except Exception as e:
            logger.error(f"[DailyDigest] Tavily fallback failed: {e}")
            return False, str(e), 0

    async def get_recent_posts(self, keyword: str, hours: int = 24):
        """
        Fetch posts for the given keyword from the last N hours.
        """
        try:
            # Calculate time threshold (milliseconds timestamp)
            time_threshold = int((datetime.now() - timedelta(hours=hours)).timestamp() * 1000)
            
            async with get_session() as session:
                if not session:
                    logger.error("Failed to get database session")
                    return []

                # Query WeiboNote (which stores Reddit data)
                # Strict Filtering: Only include posts created within the time window
                # We use create_time (post publish time) instead of add_ts (crawl time) for accuracy
                stmt = select(WeiboNote).where(
                    and_(
                        WeiboNote.source_keyword == keyword,
                        WeiboNote.create_time >= time_threshold
                    )
                ).order_by(WeiboNote.create_time.desc())
                
                result = await session.execute(stmt)
                posts = result.scalars().all()
                
                logger.info(f"Found {len(posts)} posts for keyword '{keyword}' in the last {hours} hours")
                
                # è¯Šæ–­é€»è¾‘ï¼šå¦‚æžœæ²¡æ‰¾åˆ°å¸–å­ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ä½†å…³é”®è¯ä¸åŒ¹é…
                if not posts:
                    logger.info("No posts found matching keyword strictly. Running diagnostics...")
                    
                    # 1. æ£€æŸ¥æœ€è¿‘1å°æ—¶æ˜¯å¦æœ‰ä»»ä½•æ•°æ®æ’å…¥
                    diag_stmt = select(WeiboNote).order_by(WeiboNote.add_ts.desc()).limit(5)
                    diag_res = await session.execute(diag_stmt)
                    recent_posts = diag_res.scalars().all()
                    
                    if recent_posts:
                        logger.info(f"Diagnostics: Found {len(recent_posts)} recent posts in DB (ignoring keyword):")
                        for p in recent_posts:
                            logger.info(f" - ID: {p.note_id}, Keyword: '{p.source_keyword}', TS: {p.add_ts}, Time: {datetime.fromtimestamp(p.add_ts/1000)}")
                    else:
                        logger.warning("Diagnostics: DB is empty or no recent posts found at all. Crawler might have failed.")
                
                return posts
        except Exception as e:
            logger.exception(f"Error fetching posts: {e}")
            return []

    def format_posts_for_llm(self, posts):
        """
        Format posts into a text string for the LLM.
        ä¿æŠ¤éšç§ï¼šä¸åŒ…å«ç”¨æˆ·IDå’Œæ¥æºä¿¡æ¯
        """
        if not posts:
            return "No posts found."
            
        formatted_text = ""
        for i, post in enumerate(posts[:50]): # é™åˆ¶åˆ°50æ¡å¸–å­é¿å…è¶…è¿‡tokené™åˆ¶
            # æ•°æ®æ˜ å°„: 
            # content -> æ ‡é¢˜ + å†…å®¹
            # liked_count -> è¯„åˆ†/ç‚¹èµžæ•°
            # comments_count -> è¯„è®ºæ•°
            # ä¸ºä¿æŠ¤éšç§ï¼Œä¸æ˜¾ç¤ºä½œè€…ä¿¡æ¯
            
            formatted_text += f"å¸–å­ {i+1}:\n"
            formatted_text += f"å†…å®¹: {post.content}\n"
            formatted_text += f"äº’åŠ¨æ•°æ®: {post.liked_count}èµž, {post.comments_count}è¯„è®º\n"
            formatted_text += "-" * 20 + "\n"
            
        return formatted_text

    async def generate_digest(self, keyword: str, hours: int = 24):
        """
        Generate the daily digest for the keyword.
        """
        # 1. Fetch posts
        posts = await self.get_recent_posts(keyword, hours)
        
        if not posts:
            return {
                "success": False,
                "message": f"No posts found for keyword '{keyword}' in the last {hours} hours. Please run the crawler first."
            }
            
        # 2. Format for LLM
        posts_text = self.format_posts_for_llm(posts)
        
        # 3. Construct Prompt
        prompt = DAILY_DIGEST_PROMPT.format(keyword=keyword, hours=hours, posts_text=posts_text)
        
        # 4. Call LLM
        try:
            logger.info(f"Generating summary for '{keyword}'...")
            logger.info(f"Prompt length: {len(prompt)} characters")
            
            import time
            start_time = time.time()
            response_text = self.llm.chat(prompt)
            end_time = time.time()
            logger.info(f"LLM call took {end_time - start_time:.2f} seconds")
            
            # Parse JSON from the end
            import json
            import re
            
            summary = response_text
            cover_card_data = {}
            
            try:
                # Robust extraction: Split by the start of the JSON code block
                if "```json" in response_text:
                    parts = response_text.split("```json")
                    summary = parts[0].strip()
                    json_text = parts[-1].split("```")[0].strip() # Take content between ```json and next ```
                    try:
                        cover_card_data = json.loads(json_text)
                    except:
                         # Fallback if json is malformed, try finding brace
                         match = re.search(r'(\{.*\})', json_text, re.DOTALL)
                         if match:
                             cover_card_data = json.loads(match.group(1))
                else:
                    # Fallback logic if no code block found
                    last_brace_idx = response_text.rfind('{')
                    if last_brace_idx != -1:
                        json_text = response_text[last_brace_idx:]
                        if json_text.strip().endswith("}"):
                             try:
                                 cover_card_data = json.loads(json_text)
                                 summary = response_text[:last_brace_idx].strip()
                             except:
                                 pass
            except Exception as e:
                logger.warning(f"Failed to parse cover card JSON: {e}")

            return {
                "success": True,
                "summary": summary,
                "cover_card": cover_card_data,
                "post_count": len(posts),
                "top_posts": [
                    {
                        "content": p.content[:100] + "...", 
                        "score": p.liked_count, 
                        "comments": p.comments_count,
                        "url": p.note_url
                    } 
                    for p in sorted(posts, key=lambda x: int(x.liked_count or 0), reverse=True)[:5]
                ]
            }
        except Exception as e:
            logger.exception(f"Error generating summary: {e}")
            return {
                "success": False,
                "message": f"Error generating summary: {str(e)}"
            }

# Helper functions for synchronous execution (e.g. from Streamlit)
def run_crawl(keyword: str, max_count: int = 100):
    """
    åŒæ­¥æ‰§è¡Œçˆ¬å–
    è¿”å›ž: (success: bool, message: str, post_count: int)
    """
    clear_engine_cache()
    digest = DailyDigest()
    return asyncio.run(digest.crawl_reddit(keyword, max_count))

def run_digest_generation(keyword: str, hours: int = 24):
    """
    åŒæ­¥æ‰§è¡Œæ‘˜è¦ç”Ÿæˆ
    """
    # Clear engine cache to avoid "attached to a different loop" error
    # because asyncio.run creates a new loop each time
    clear_engine_cache()
    digest = DailyDigest()
    result = asyncio.run(digest.generate_digest(keyword, hours))
    
    # å¦‚æžœç”ŸæˆæˆåŠŸï¼Œä¿å­˜åˆ°åŽ†å²è®°å½•
    if result.get('success'):
        try:
            from DailyDigest.models import save_digest_history
            success, history_id = save_digest_history(keyword, result)
            if success:
                logger.info(f"Saved digest history with ID: {history_id}")
                result['history_id'] = history_id
        except Exception as e:
            logger.warning(f"Failed to save history: {e}")
    
    return result

def run_crawl_and_digest(keyword: str, hours: int = 24, max_count: int = 100):
    """
    ä¸€é”®æ‰§è¡Œï¼šçˆ¬å– + ç”Ÿæˆæ‘˜è¦
    è¿”å›ž: {
        "crawl_success": bool,
        "crawl_message": str,
        "post_count": int,
        "digest_result": dict  # æ‘˜è¦ç»“æžœ
    }
    """
    # Step 1: çˆ¬å–
    crawl_success, crawl_message, post_count = run_crawl(keyword, max_count)
    
    if not crawl_success:
        return {
            "crawl_success": False,
            "crawl_message": crawl_message,
            "post_count": 0,
            "digest_result": {
                "success": False,
                "message": "çˆ¬å–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"
            }
        }
    
    # Step 2: ç”Ÿæˆæ‘˜è¦
    digest_result = run_digest_generation(keyword, hours)
    
    return {
        "crawl_success": True,
        "crawl_message": crawl_message,
        "post_count": post_count,
        "digest_result": digest_result
    }

if __name__ == "__main__":
    # Test run
    if len(sys.argv) > 1:
        kw = sys.argv[1]
        print(run_digest_generation(kw))
    else:
        print("Please provide a keyword")
